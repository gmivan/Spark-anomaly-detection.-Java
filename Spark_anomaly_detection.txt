package com.anomaly;

import java.util.Stack;

import static spark.Spark.*;

public class Server {

    private static final Stack<Entry> streamOfResults = new Stack<>();
    private static long total = 0;
    private static long anomaly = 0;

    public static void main(String[] args) throws Throwable {
        startServer();

        Integer port = 8999;
        String pathToData = "/home/cloudera/kdd/kddCup3.data";
        Long timeout = 10L;
        if (args.length > 0) {
            if (!args[0].isEmpty()) {
                port = Integer.valueOf(args[0]);
            }
            if (!args[1].isEmpty()) {
                pathToData = args[1];
            }
            if (!args[2].isEmpty()) {
                timeout = Long.valueOf(args[2]);
            }
        }

        RunKMeans.findAnomalies(port, pathToData, timeout, streamOfResults);
    }

    public static void startServer() {
        staticFileLocation("/public");

        get("/index", (req, res) -> {
            return  "<!DOCTYPE html>\n" +
                    "<html>\n" +
                    "<head lang=\"en\">\n" +
                    "    <meta charset=\"UTF-8\">\n" +
                    "    <title></title>\n" +
                    "    <script src=\"amcharts.js\" type=\"text/javascript\"></script>\n" +
                    "    <script src=\"jquery-2.1.4.js\" type=\"text/javascript\"></script>\n" +
                    "    <script src=\"graphic.js\" type=\"text/javascript\"></script>\n" +
                    "<style type=\"text/css\">\n" +
                    "   .block { \n" +
                    "    margin:0 0 0 0;" +
                    "    padding:0 0 0 0;\n" +
                    "   }\n" +
                    "  </style> " +
                    "</head>\n" +
                    "<body>\n" +
                    "</head>\n" +
                    "<body>\n" +
                    "<div id=\"chartdiv\" style=\"width: 100%; height: 400px; background-color: #FFFFFF;\" ></div>\n" +
                    "<div id=\"summarydiv\"><p class=\"block\"><b>Summary</b></p>\n" +
                    "<div id=\"totaldiv\"><p class=\"block\">Total: 0</p></div>\n" +
                    "<div id=\"anomalydiv\"><p class=\"block\">Anomaly: 0</p></div></div>\n" +
                    "</body>\n" +
                    "</html>";
        });

        get("/data", "application/json", (req, res) -> {
            return  "{\n" +
                    "    \"total\": 4,\n" +
                    "    \"anomaly\": 2\n" +
                    "}";
        });


        get("/anomaly", (req, res) -> {
            Entry entry = null;
            if (!streamOfResults.empty()) {
                entry = streamOfResults.pop();
            }

            total = entry != null ? entry.getTotalCount() : total;
            anomaly = entry != null ? entry.getAnomalyCount() : anomaly;

            return  "{\n" +
                    "    \"total\": " + total + ",\n" +
                    "    \"anomaly\": " + anomaly + "\n" +
                    "}";

        });
    }
}

package com.anomaly;

public class Entry {
    private long totalCount;
    Private long anomalycount;

    public Entry(long totalCount, long anomalyCount) {
        this.totalCount = totalCount;
        this.anomalyCount = anomalyCount;
    }

    public long getTotalCount() {
        return totalCount;
    }

    public void setTotalCount(long totalCount) {
        this.totalCount = totalCount;
    }

    public long getAnomalyCount() {
        return anomalyCount;
    }

    public void setAnomalyCount(long anomalyCount) {
        this.anomalyCount = anomalyCount;
    }
}

package com.anomaly;

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.mllib.clustering.KMeans;
import org.apache.spark.mllib.clustering.KMeansModel;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.mllib.linalg.Vector;
import org.apache.spark.mllib.linalg.Vectors;

public class AnomalyTest {

    public static void main(String[] args) {

        SparkConf conf = new SparkConf().setMaster("local").setAppName("AnomalyDetection");
        SparkContext sparkContext = new SparkContext(conf);
        JavaSparkContext context = new JavaSparkContext(sparkContext);
        JavaStreamingContext streamingContext = new JavaStreamingContext(context, new Duration(1000));

        // Load and parse data
        String path = "/home/cloudera/OryxUpdate/kddCup.data";
        JavaRDD<String> data = context.textFile(path);
        JavaRDD<Vector> parsedData = data.map(
                new Function<String, Vector>() {
                    public Vector call(String s) {
                        String[] sarray = s.split(",");
                        double[] values = new double[sarray.length];
                        for (int i = 0; i < sarray.length; i++)
                            values[i] = Double.parseDouble(sarray[i]);
                        return Vectors.dense(values);
                    }
                }
        );
        parsedData.cache();

        // Cluster the data into two classes using KMeans
        int numClusters = 2;
        int numIterations = 20;
        KMeansModel clusters = KMeans.train(parsedData.rdd(), numClusters, numIterations);

        // Evaluate clustering by computing Within Set Sum of Squared Errors
        double WSSSE = clusters.computeCost(parsedData.rdd());
        System.out.println("Within Set Sum of Squared Errors = " + WSSSE);

        System.out.println("parsed Data: " + parsedData.count());
    }
}

package com.anomaly

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD

object AnomalyStreaming {
  def main (args: Array[String]) {
    val sparkConf = new SparkConf()
      .setMaster("local[2]")
      .setAppName("AnomalyStreaming")
    val sparkContext = new SparkContext(sparkConf)
    val sparkStreamContext = new StreamingContext(sparkContext, Seconds(10))
    val text = sparkStreamContext.socketTextStream("localhost", 9999)
    text.print()
    sparkStreamContext.start()
    sparkStreamContext.awaitTermination()
  }
}

package com.anomaly

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD

object AnomalyDetection {

  def main (args: Array[String]) {
    findAnomaly()
  }

  def findAnomaly(){

    val sparkConf = new SparkConf()
      .setMaster("local")
      //.setMaster("quickstart.cloudera:18080")
      .setAppName("AnomalyDetection")
    //.set("spark.driver.host", "localhost")
    //.set("spark.driver.port", "7077")
    val sparkContext = new SparkContext(sparkConf)
   /* val sparkStreamContext = new StreamingContext(sparkContext, Seconds(10))
    val text = sparkStreamContext.socketTextStream("localhost", 9999)
    text.print()
    sparkStreamContext.start()
    sparkStreamContext.awaitTermination()*/

    val fileName: String = "/home/cloudera/OryxUpdate/kddCup2.data"


    val rawData = sparkContext.textFile(fileName)

    //rawData.take(1)
    val dataAndLabel = rawData.map { line =>
      val buffer = line.split(",").toBuffer
      buffer.remove(1, 3)
      val label = buffer.remove(buffer.length - 1)
      val vector = buffer.map(_.toDouble).toArray
      (vector, label)
    }


    val data = dataAndLabel.map(_._1).cache()

    val numCols = data.take(1)(0).length
    val n = data.count
    val sums = data.reduce((a,b) => a.zip(b).map(t => t._1 + t._2))
    val sumSquares = data.fold(new Array[Double](numCols))((a, b) => a.zip(b).map(t => t._1 + t._2 * t._2))

    val stdevs = sumSquares.zip(sums).map{
      case(sumSq, sum) => Math.sqrt(n * sumSq - sum*sum)/n
    }

    val means = sums.map(_/n)

    def normalize(f: Array[Double]) = (f, means, stdevs).zipped.map((value, mean, stdev) =>
      if(stdev <= 0) (value-mean) else (value-mean)/stdev)

    val normalizedDataVector = data.map(s => Vectors.dense(s)).cache()

    //anomaly Detection
    val kMeans = new KMeans()
    kMeans.setK(95)
    kMeans.setRuns(10)
    kMeans.setEpsilon(1.0e-6)
    val model = kMeans.run(normalizedDataVector)

    val distances = normalizedDataVector.map(datum => (distToCentroid(datum, model), datum))

    val outliners = distances.top(100)(Ordering.by(_._1))
    val treshold = outliners.last._1

    println(outliners)
    println("---------------")
    println(treshold)

    def anomaly(datum: Array[Double], model: KMeansModel) = distToCentroid(Vectors.dense(normalize(datum)), model) > treshold
  }

  def distance(a: Vector, b: Array[Double]) = Math.sqrt(a.toArray.zip(b).map(p => p._1 - p._2).map(d => d*d).sum)

  def distToCentroid(datum: Vector, model: KMeansModel) = distance(model.clusterCenters(model.predict(datum)), datum.toArray)

  def clusteringScore(data: RDD[Vector], k: Int) =
  {
    val kMeans = new KMeans()
    kMeans.setK(k)
    val model = kMeans.run(data)

    data.map(datum => distToCentroid(datum, model)).mean()
  }
}

package com.anomaly

import org.apache.spark.mllib.clustering._
import org.apache.spark.mllib.linalg._
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import java.util.Stack

object RunKMeans {

  var originalRawData : RDD[String] = null
  var modelTrained = false
  var model = new KMeansModel(null)
  var threshold = Double.MinValue
  var allAnomalies : RDD[String] = null

  def main(args: Array[String]): Unit = {
    findAnomalies(Integer.valueOf(args(0)), args(1), args(2).asInstanceOf[Long], null)
  }

  // Clustering, Take 0

  def findAnomalies(port : Integer, pathToData : String, timeout : Long, streamOfResults : Stack[Entry]): Unit = {
    val sc = new SparkContext(new SparkConf().setMaster("local[2]").setAppName("K-means"))
    originalRawData = sc.textFile(pathToData)

    /*    clusteringTake0(rawData)
    clusteringTake1(rawData)
    clusteringTake2(rawData)
    clusteringTake3(rawData)
    clusteringTake4(rawData)*/
    anomalies(originalRawData)

    fetchData(modelTrained, sc, port, timeout, streamOfResults)
  }

  def fetchData(modelTrained: Boolean, sc: SparkContext, port: Integer, timeout: Long, streamOfResults : Stack[Entry]): Unit = {
    val ssc = new StreamingContext(sc, Seconds(timeout))
    val newData = ssc.socketTextStream("localhost", port)

    if (modelTrained) {
      newData.print()
      if (newData.toString != null) {
        newData.foreachRDD(rdd => {
          if (!rdd.isEmpty()) {
            detectAnomaly(rdd, streamOfResults)
          }
        })
      }
    }

    ssc.start()
    ssc.awaitTermination()
  }

  def clusteringTake0(rawData: RDD[String]): Unit = {

    rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).reverse.foreach(println)

    val labelsAndData = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      val label = buffer.remove(buffer.length - 1)
      val vector = Vectors.dense(buffer.map(_.toDouble).toArray)
      (label, vector)
    }

    val data = labelsAndData.values.cache()

    val kmeans = new KMeans()
    val model = kmeans.run(data)

    model.clusterCenters.foreach(println)

    val clusterLabelCount = labelsAndData.map { case (label, datum) =>
      val cluster = model.predict(datum)
      (cluster, label)
    }.countByValue()

    clusterLabelCount.toSeq.sorted.foreach { case ((cluster, label), count) =>
      println(f"$cluster%1s$label%18s$count%8s")
    }

    data.unpersist()
  }

  // Clustering, Take 1

  def distance(a: Vector, b: Vector) =
    math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)

  def distToCentroid(datum: Vector, model: KMeansModel) = {
    val cluster = model.predict(datum)
    val centroid = model.clusterCenters(cluster)
    distance(centroid, datum)
  }

  def clusteringScore(data: RDD[Vector], k: Int): Double = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    val model = kmeans.run(data)
    data.map(datum => distToCentroid(datum, model)).mean()
  }

  def clusteringScore2(data: RDD[Vector], k: Int): Double = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(data)
    data.map(datum => distToCentroid(datum, model)).mean()
  }

  def clusteringTake1(rawData: RDD[String]): Unit = {

    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }.cache()

    (5 to 30 by 5).map(k => (k, clusteringScore(data, k))).
      foreach(println)

    (30 to 100 by 10).par.map(k => (k, clusteringScore2(data, k))).
      toList.foreach(println)

    data.unpersist()

  }

  def visualizationInR(rawData: RDD[String]): Unit = {

    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }.cache()

    val kmeans = new KMeans()
    kmeans.setK(100)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    val model = kmeans.run(data)

    val sample = data.map(datum =>
      model.predict(datum) + "," + datum.toArray.mkString(",")
    ).sample(false, 0.05)

    sample.saveAsTextFile("hdfs:///user/ds/sample")

    data.unpersist()

  }

  // Clustering, Take 2

  def buildNormalizationFunction(data: RDD[Vector]): (Vector => Vector) = {
    val dataAsArray = data.map(_.toArray)
    val numCols = dataAsArray.first().length
    val n = dataAsArray.count()
    val sums = dataAsArray.reduce(
      (a, b) => a.zip(b).map(t => t._1 + t._2))
    val sumSquares = dataAsArray.fold(
      new Array[Double](numCols)
    )(
        (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2)
      )
    val stdevs = sumSquares.zip(sums).map {
      case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n
    }
    val means = sums.map(_ / n)

    (datum: Vector) => {
      val normalizedArray = (datum.toArray, means, stdevs).zipped.map(
        (value, mean, stdev) =>
          if (stdev <= 0)  (value - mean) else  (value - mean) / stdev
      )
      Vectors.dense(normalizedArray)
    }
  }

  def clusteringTake2(rawData: RDD[String]): Unit = {
    val data = rawData.map { line =>
      val buffer = line.split(',').toBuffer
      buffer.remove(1, 3)
      buffer.remove(buffer.length - 1)
      Vectors.dense(buffer.map(_.toDouble).toArray)
    }

    val normalizedData = data.map(buildNormalizationFunction(data)).cache()

    (60 to 120 by 10).par.map(k =>
      (k, clusteringScore2(normalizedData, k))).toList.foreach(println)

    normalizedData.unpersist()
  }

  // Clustering, Take 3

  def buildCategoricalAndLabelFunction(rawData: RDD[String]): (String => (String,Vector)) = {
    val splitData = rawData.map(_.split(','))
    val protocols = splitData.map(_(1)).distinct().collect().zipWithIndex.toMap
    val services = splitData.map(_(2)).distinct().collect().zipWithIndex.toMap
    val tcpStates = splitData.map(_(3)).distinct().collect().zipWithIndex.toMap
    (line: String) => {
      val buffer = line.split(',').toBuffer
      val protocol = buffer.remove(1)
      val service = buffer.remove(1)
      val tcpState = buffer.remove(1)
      val label = buffer.remove(buffer.length - 1)
      val vector = buffer.map(_.toDouble)

      val newProtocolFeatures = new Array[Double](protocols.size)
      newProtocolFeatures(protocols(protocol)) = 1.0
      val newServiceFeatures = new Array[Double](services.size)
      newServiceFeatures(services(service)) = 1.0
      val newTcpStateFeatures = new Array[Double](tcpStates.size)
      newTcpStateFeatures(tcpStates(tcpState)) = 1.0

      vector.insertAll(1, newTcpStateFeatures)
      vector.insertAll(1, newServiceFeatures)
      vector.insertAll(1, newProtocolFeatures)

      (label, Vectors.dense(vector.toArray))
    }
  }

  def clusteringTake3(rawData: RDD[String]): Unit = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val data = rawData.map(parseFunction).values
    val normalizedData = data.map(buildNormalizationFunction(data)).cache()

    (80 to 160 by 10).map(k =>
      (k, clusteringScore2(normalizedData, k))).toList.foreach(println)

    normalizedData.unpersist()
  }

  // Clustering, Take 4

  def entropy(counts: Iterable[Int]) = {
    val values = counts.filter(_ > 0)
    val n: Double = values.sum
    values.map { v =>
      val p = v / n
      -p * math.log(p)
    }.sum
  }

  def clusteringScore3(normalizedLabelsAndData: RDD[(String,Vector)], k: Int) = {
    val kmeans = new KMeans()
    kmeans.setK(k)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)

    val model = kmeans.run(normalizedLabelsAndData.values)

    // Predict cluster for each datum
    val labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)

    // Swap keys / values
    val clustersAndLabels = labelsAndClusters.map(_.swap)

    // Extract collections of labels, per cluster
    val labelsInCluster = clustersAndLabels.groupByKey().values

    // Count labels in collections
    val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))

    // Average entropy weighted by cluster size
    val n = normalizedLabelsAndData.count()

    labelCounts.map(m => m.sum * entropy(m)).sum / n
  }

  def clusteringTake4(rawData: RDD[String]): Unit = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val labelsAndData = rawData.map(parseFunction)
    val normalizedLabelsAndData =
      labelsAndData.mapValues(buildNormalizationFunction(labelsAndData.values)).cache()

    (80 to 160 by 10).map(k =>
      (k, clusteringScore3(normalizedLabelsAndData, k))).toList.foreach(println)

    normalizedLabelsAndData.unpersist()
    modelTrained = true
  }

  // Detect anomalies

  def buildAnomalyDetector(
                            data: RDD[Vector],
                            normalizeFunction: (Vector => Vector)): (Vector => Boolean) = {
    val normalizedData = data.map(normalizeFunction)
    normalizedData.cache()

    val kmeans = new KMeans()
    kmeans.setK(150)
    kmeans.setRuns(10)
    kmeans.setEpsilon(1.0e-6)
    model = kmeans.run(normalizedData)

    //TODO: streaming model
    //val clusterWeights= Array.fill(model.clusterCenters.length)(1.0)
    //val sKmeans = new StreamingKMeansModel(model.clusterCenters, clusterWeights)

    normalizedData.unpersist()

    val distances = normalizedData.map(datum => distToCentroid(datum, model))
    threshold = distances.top(100).last

    (datum: Vector) => distToCentroid(normalizeFunction(datum), model) > threshold
  }

/*  def isAnomaly(normalizeFunction: (Vector => Vector), model: KMeansModel, threshold: Double): (Vector => Boolean) = {
    (datum: Vector) => distToCentroid(normalizeFunction(datum), model) > threshold
  }*/

  def isAnomaly(data: RDD[Vector], normalizeFunction: (Vector => Vector), model: KMeansModel, threshold: Double): (Vector => Boolean) = {
    val normalizedData = data.map(normalizeFunction)
    normalizedData.cache()

    (datum: Vector) => distToCentroid(normalizeFunction(datum), model) > threshold
  }

  def buildAnomalyDetectorNew(
                            data: RDD[Vector],
                            normalizeFunction: (Vector => Vector)): (Vector => Boolean) = {
    val normalizedData = data.map(normalizeFunction)
    normalizedData.cache()

    normalizedData.unpersist()

    val distances = normalizedData.map(datum => distToCentroid(datum, model))
    threshold = distances.top(100).last

    (datum: Vector) => distToCentroid(normalizeFunction(datum), model) > threshold
  }

  def anomalies(rawData: RDD[String]) = {
    val parseFunction = buildCategoricalAndLabelFunction(rawData)
    val originalAndData = rawData.map(line => (line, parseFunction(line)._2))
    val data = originalAndData.values
    val normalizeFunction = buildNormalizationFunction(data)
    val anomalyDetector = buildAnomalyDetector(data, normalizeFunction)
    allAnomalies = originalAndData.filter {
      case (original, datum) => anomalyDetector(datum)
    }.keys

    println("------------------Anomalies------------------")
    allAnomalies.foreach(println)
    modelTrained = true
  }

  def detectAnomaly(rawData: RDD[String], streamOfResults : Stack[Entry]) = {
    val newData = rawData.union(originalRawData)
    val parseFunction = buildCategoricalAndLabelFunction(newData)
    val originalAndData = newData.map(line => (line, parseFunction(line)._2))
    val data = originalAndData.values
    val normalizeFunction = buildNormalizationFunction(data)
    val anomalyDetector = buildAnomalyDetectorNew(data, normalizeFunction)
    val anomaliesNew = originalAndData.filter {
      case (original, datum) => anomalyDetector(datum)
    }.keys

    val na = anomaliesNew.collect().intersect(rawData.collect())
    //val anomaliesDetected = anomaliesNew.collect().diff(allAnomalies.collect())//subtract(allAnomalies)

    //println("------------------Anomalies------------------")
    //anomaliesNew.foreach(println)


    println("---------------Detection--------------------")
    println("Total count:" + rawData.count())
    println("Anomalies count:" + na.length)
    //na.foreach(println)

    streamOfResults.push(new Entry(rawData.count(), na.length))
  }

}

var chart;
var chartData = [];
var chartCursor;
var totalCount = 0;
var anomalyCount = 0;

// create chart
AmCharts.ready(function() {
    // SERIAL CHART
    chart = new AmCharts.AmSerialChart();
    chart.pathToImages = "http://www.amcharts.com/lib/images/";
    chart.marginTop = 0;
    chart.marginRight = 10;
    chart.autoMarginOffset = 5;
    chart.zoomOutButton = {
        backgroundColor: '#000000',
        backgroundAlpha: 0.15
    };
    chart.dataProvider = chartData;
    chart.dataDateFormat = "YYYY-MM-DD HH:NN:SS";
    chart.categoryField = "date";

    // AXES
    // category
    var categoryAxis = chart.categoryAxis;
    categoryAxis.parseDates = true; // as our data is date-based, we set parseDates to true
    categoryAxis.minPeriod = "ss"; // our data is daily, so we set minPeriod to DD
    categoryAxis.dashLength = 1;
    categoryAxis.gridAlpha = 0.15;
    categoryAxis.axisColor = "#DADADA";

    // value
    var valueAxis = new AmCharts.ValueAxis();
    valueAxis.axisAlpha = 0.2;
    valueAxis.dashLength = 1;
    chart.addValueAxis(valueAxis);

    // GRAPH
    var graph = new AmCharts.AmGraph();
    graph.title = "red line";
    graph.valueField = "total";
    graph.bullet = "round";
    graph.bulletBorderColor = "#FFFFFF";
    graph.bulletBorderThickness = 2;
    graph.lineThickness = 2;
    graph.lineColor = "#0352b5";
    //graph.negativeLineColor = "#0352b5";
    //graph.hideBulletsCount = 50; // this makes the chart to hide bullets when there are more than 50 series in selection
    chart.addGraph(graph);

     var graph2 = new AmCharts.AmGraph();
        graph2.title = "red line";
        graph2.valueField = "anomaly";
        graph2.bullet = "round";
        graph2.bulletBorderColor = "#FFFFFF";
        graph2.bulletBorderThickness = 2;
        graph2.lineThickness = 2;
        graph2.lineColor = "#b5030d";
        //graph.negativeLineColor = "#0352b5";
        //graph.hideBulletsCount = 50; // this makes the chart to hide bullets when there are more than 50 series in selection
        chart.addGraph(graph2);

    // CURSOR
    chartCursor = new AmCharts.ChartCursor();
    chartCursor.cursorPosition = "mouse";
    chart.addChartCursor(chartCursor);

    // SCROLLBAR
    var chartScrollbar = new AmCharts.ChartScrollbar();
    chartScrollbar.graph = graph;
    chartScrollbar.scrollbarHeight = 40;
    chartScrollbar.color = "#FFFFFF";
    chartScrollbar.autoGridCount = true;
    chart.addChartScrollbar(chartScrollbar);

    // WRITE
    chart.write("chartdiv");

    // set up the chart to update every second
    setInterval(function () {
        //chart.dataProvider.shift();

        $.getJSON("/anomaly", function( json ) {
          chart.dataProvider.push({
                      date: new Date(),
                      total: json.total,
                      anomaly: json.anomaly
                  });
          //console.log( "JSON Data: " + json.total );
          totalCount += json.total;
          anomalyCount += json.anomaly;
        });

        $(document).ready(function(){
            $('#totaldiv').html('<p class="block">Total: ' + totalCount + '</p></div>');
            if (anomalyCount > 30) {
                $('#anomalydiv').html('<p class="block">Anomaly: <span style="color: red;">' + anomalyCount + '</span></p></div>');
            } else {
                $('#anomalydiv').html('<p class="block">Anomaly: ' + anomalyCount + '</p></div>');
            }
        });

        chart.validateData();
    }, 5000);
});
Распечатать на бумаге А4, с одной стороны.

Сшить нитками.

Поверх ниток приклеить такую наклейку (см. ниже) и подписать ее.



 

Всего пронумеровано и прошнуровано
___ листов фрагментов исходного текста программы

Правообладатель


________________ /__________________
                подпись 	   
